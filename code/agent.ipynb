{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f819959e",
   "metadata": {},
   "source": [
    "## RAG Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "505bd0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: ['cuda:0']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.messages import AIMessage, SystemMessage\n",
    "from langchain_core.prompts import HumanMessagePromptTemplate, ChatPromptTemplate, AIMessagePromptTemplate\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from langgraph.graph import START, StateGraph\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Filter, FieldCondition, MatchValue, Distance, VectorParams\n",
    "from typing_extensions import List, TypedDict, Annotated, Literal\n",
    "\n",
    "#---------------------------------------------------------------------------------------\n",
    "# set parameters\n",
    "#---------------------------------------------------------------------------------------\n",
    "# general parameters\n",
    "user_ID = \"user_1\" # user ID for the chat history\n",
    "\n",
    "# Qdrant parameters\n",
    "document_collection_name = \"demo_collection\" # name of the company documents Qdrant collection\n",
    "product_collection_name = \"product_collection\" # name of the company documents Qdrant collection\n",
    "history_collection_name = \"history_collection\" # name of the company documents Qdrant collection\n",
    "db_path = \"test_rag_db\" # path to the Qdrant database\n",
    "distance = Distance.COSINE # distance metric for the Qdrant database\n",
    "\n",
    "# embedding model parameters\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\" # embedding model name\n",
    "\n",
    "# LLM parameters\n",
    "llm_name = \"llama3.1:8b\" # name of the LLM model\n",
    "k = 5 # number of documents to retrieve from the RAG database\n",
    "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\n",
    "#---------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------------------------\n",
    "# connect to Qdrant database and vector stores\n",
    "#----------------------------------------------------------------------------------------\n",
    "# embedding model used for the Qdrant database\n",
    "embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
    "\n",
    "# Initialize the Qdrant client with a local path\n",
    "db_client = QdrantClient(path=db_path)\n",
    "\n",
    "if not db_client.collection_exists(document_collection_name) or not db_client.collection_exists(product_collection_name):\n",
    "    raise ValueError(f\"Collection {document_collection_name} or {product_collection_name} does not exist.\")\n",
    "\n",
    "document_vector_store = QdrantVectorStore(\n",
    "    client=db_client,\n",
    "    collection_name=document_collection_name,\n",
    "    embedding=embeddings,\n",
    ")\n",
    "\n",
    "product_vector_store = QdrantVectorStore(\n",
    "    client=db_client,\n",
    "    collection_name=product_collection_name,\n",
    "    embedding=embeddings,\n",
    ")\n",
    "\n",
    "if not db_client.collection_exists(history_collection_name):\n",
    "    # create history collection if it doesn't exist\n",
    "\n",
    "    vector_length = len(embeddings.embed_documents([\"dummy\"])[0])  # get vector length from dummy embedding\n",
    "    db_client.create_collection(\n",
    "        collection_name=history_collection_name,\n",
    "        vectors_config=VectorParams(size=vector_length, distance=distance),\n",
    "    )\n",
    "\n",
    "history_vector_store = QdrantVectorStore(\n",
    "    client=db_client,\n",
    "    collection_name=history_collection_name,\n",
    "    embedding=embeddings,\n",
    ")\n",
    "#-----------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------------------------\n",
    "# set up promt + LLM and load companies + chat history\n",
    "#---------------------------------------------------------------------------------------\n",
    "# Load comanies \n",
    "with open(\"companies.json\") as f:\n",
    "    companies = json.load(f)\n",
    "\n",
    "# Load chat history for the user\n",
    "chat_history = db_client.scroll(\n",
    "    collection_name=history_collection_name,\n",
    "    scroll_filter=Filter(must=[FieldCondition(key=\"user_ID\", match=MatchValue(value=user_ID))]),\n",
    "    limit=1,\n",
    ")\n",
    "chat_history = chat_history[0][0].payload[\"chat_history\"] if chat_history and len(chat_history[0]) > 0 else \"\"\n",
    "\n",
    "# Define prompt for question-answering\n",
    "system_message = SystemMessage(\"You are a helpful AI assistant for question-answering tasks. The provided chat history includes facts about the user you are speaking with.\")\n",
    "history_message = AIMessagePromptTemplate.from_template(\"Chat history: {chat_history}\") \n",
    "input_message = HumanMessagePromptTemplate.from_template(\"\"\"Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know.\n",
    "                                                            Question: {question} Please provide a detailed answer to the question containing all the information provided in the context.\n",
    "                                                            Context: {context} \n",
    "                                                            Answer:\"\"\") # create input message\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([system_message, history_message, input_message]) # create prompt template\n",
    "\n",
    "# Define LLM for question-answering\n",
    "device = [\"cuda:0\"] if torch.cuda.is_available() else [\"cpu\"]\n",
    "print(f\"Using device: {device}\")\n",
    "llm = ChatOllama(model=llm_name)\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------------------------------\n",
    "# Define state graph for the application\n",
    "#---------------------------------------------------------------------------------------\n",
    "# define Search query structure\n",
    "class Search(TypedDict):\n",
    "\n",
    "    query: Annotated[str, ..., \"Search query to run.\"]\n",
    "    company: Annotated[\n",
    "        #Literal[tuple(companies.keys())],     # list of all companies in the database (add all companies here), ev also add topic filter\n",
    "        Literal[\"Beiersdorf\", \"L'OrÃ©al\"],     # list of all companies in the database (add all companies here), ev also add topic filter\n",
    "        ...,\n",
    "        \"company to query.\",\n",
    "    ]\n",
    "    topic: Annotated[\n",
    "        Literal[\"location,workforce,shareholders\", \"ethics\", \"environment\"],     # list of topics\n",
    "        ...,\n",
    "        \"topic the query is about.\",\n",
    "    ]\n",
    "\n",
    "# Define state for application\n",
    "class State(TypedDict):\n",
    "    chat_history : str\n",
    "    question: str\n",
    "    query: Search\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "    documet_db: QdrantVectorStore \n",
    "    product_db: QdrantVectorStore \n",
    "    history_db: QdrantVectorStore \n",
    "    client: QdrantClient \n",
    "    \n",
    "# Define application steps\n",
    "def analyze_query(state: State):\n",
    "    \"\"\"\n",
    "    Extract the company and topic from the question and create a structured query.\n",
    "    \"\"\"\n",
    "\n",
    "    print(state[\"chat_history\"])\n",
    "    structured_llm = llm.with_structured_output(Search)\n",
    "    #TODO properly input the chat history and question\n",
    "    query = structured_llm.invoke(state[\"chat_history\"] + \"\\n\\n\" + state[\"question\"])\n",
    "\n",
    "    # map company info to location, workforce, shareholders (as this is how it is stored in the database but there is less ambiguity using the other term)\n",
    "    if query[\"topic\"] == \"location,workforce,shareholders\":\n",
    "        query[\"topic\"] = \"company info\"\n",
    "\n",
    "    return {\"query\": query}\n",
    "\n",
    "def retrieve(state: State):\n",
    "    \"\"\"\n",
    "    Retrieve relevant documents from the document database based on the structured query.\n",
    "    \"\"\"\n",
    "    query = state[\"query\"]\n",
    "    retrieved_docs = state[\"documet_db\"].similarity_search(\n",
    "        query[\"query\"],\n",
    "        filter=Filter(must=[FieldCondition(key=\"metadata.company\", match=MatchValue(value=query[\"company\"])),\n",
    "                            FieldCondition(key=\"metadata.topic\", match=MatchValue(value=query[\"topic\"])),\n",
    "                            ]),\n",
    "        k=k,\n",
    "    )\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "def generate(state: State):\n",
    "    \"\"\"\n",
    "    Generate an answer based on the retrieved documents, the chat history and the question.\n",
    "    \"\"\"\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \n",
    "                              \"context\": docs_content,\n",
    "                              \"chat_history\": state[\"chat_history\"]})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "def summarize(state: State):\n",
    "    \"\"\"\n",
    "    Summarize the previous chat history, question, and answer into a new chat history.\n",
    "    \"\"\"\n",
    "    summarization_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            state[\"chat_history\"],\n",
    "            state[\"question\"],\n",
    "            state[\"answer\"],\n",
    "            (\n",
    "                \"user\",\n",
    "                \"Distill the above chat messages into a single summary message. Include especially user preferences, company names and product names and if there is conflicting informatino save the most recent one.\",\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    summary = llm.invoke(summarization_prompt.invoke({})).content\n",
    "    return {\"chat_history\": summary}\n",
    "\n",
    "def save_history(state: State):\n",
    "    \"\"\"\n",
    "    Save the chat history to the history database (using the user_id).\n",
    "    \"\"\"\n",
    "    state[\"client\"].set_payload(\n",
    "        collection_name=state[\"history_db\"].collection_name,\n",
    "        payload={\"user_ID\": user_ID, \"chat_history\": state[\"chat_history\"]},\n",
    "        points=Filter(must=[FieldCondition(key=\"user_ID\", match=MatchValue(value=user_ID))]),\n",
    "    )\n",
    "\n",
    "# build and compile the state graph\n",
    "graph_builder = StateGraph(State).add_sequence([analyze_query, retrieve, generate, summarize, save_history])\n",
    "graph_builder.add_edge(START, \"analyze_query\")\n",
    "graph = graph_builder.compile()\n",
    "#---------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# function to run the state graph with predefined database\n",
    "def invoke_graph(graph: StateGraph, question: str, chat_history: str, \n",
    "                 db_client: QdrantClient = db_client, \n",
    "                 document_db: QdrantVectorStore = document_vector_store, \n",
    "                 product_db: QdrantVectorStore = product_vector_store, \n",
    "                 history_db: QdrantVectorStore = history_vector_store) -> str:\n",
    "    \"\"\"\n",
    "    Run the state graph with the given question and chat history.\n",
    "\n",
    "    Params:\n",
    "        graph (StateGraph): The state graph to run.\n",
    "        question (str): The question asked by the user.\n",
    "        chat_history (str): The chat history for the user.\n",
    "        Optional:\n",
    "            db_client (QdrantClient): The Qdrant client to use.\n",
    "            document_db (QdrantVectorStore): The document vector store to use.\n",
    "            product_db (QdrantVectorStore): The product vector store to use.\n",
    "            history_db (QdrantVectorStore): The history vector store to use.\n",
    "    \"\"\"\n",
    "    state = State(\n",
    "        chat_history=chat_history,\n",
    "        question=question,\n",
    "        query={},\n",
    "        context=[],\n",
    "        answer=\"\",\n",
    "        client=db_client,\n",
    "        documet_db=document_db,\n",
    "        product_db=product_db,\n",
    "        history_db=history_db,\n",
    "    )\n",
    "    return graph.invoke(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e922af1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:25:29\n",
      "\n"
     ]
    },
    {
     "ename": "RemoteProtocolError",
     "evalue": "Server disconnected without sending a response.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRemoteProtocolError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Anwender\\anaconda3\\envs\\RAG\\lib\\site-packages\\httpx\\_transports\\default.py:101\u001b[0m, in \u001b[0;36mmap_httpcore_exceptions\u001b[1;34m()\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 101\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32mc:\\Users\\Anwender\\anaconda3\\envs\\RAG\\lib\\site-packages\\httpx\\_transports\\default.py:250\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[1;32m--> 250\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n",
      "File \u001b[1;32mc:\\Users\\Anwender\\anaconda3\\envs\\RAG\\lib\\site-packages\\httpcore\\_sync\\connection_pool.py:256\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[1;32m--> 256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Anwender\\anaconda3\\envs\\RAG\\lib\\site-packages\\httpcore\\_sync\\connection_pool.py:236\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[1;32m--> 236\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[0;32m    242\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    243\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Anwender\\anaconda3\\envs\\RAG\\lib\\site-packages\\httpcore\\_sync\\connection.py:103\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Anwender\\anaconda3\\envs\\RAG\\lib\\site-packages\\httpcore\\_sync\\http11.py:136\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    135\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[1;32m--> 136\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[1;32mc:\\Users\\Anwender\\anaconda3\\envs\\RAG\\lib\\site-packages\\httpcore\\_sync\\http11.py:106\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[0;32m     99\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[0;32m    100\u001b[0m     (\n\u001b[0;32m    101\u001b[0m         http_version,\n\u001b[0;32m    102\u001b[0m         status,\n\u001b[0;32m    103\u001b[0m         reason_phrase,\n\u001b[0;32m    104\u001b[0m         headers,\n\u001b[0;32m    105\u001b[0m         trailing_data,\n\u001b[1;32m--> 106\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_receive_response_headers(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    107\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    108\u001b[0m         http_version,\n\u001b[0;32m    109\u001b[0m         status,\n\u001b[0;32m    110\u001b[0m         reason_phrase,\n\u001b[0;32m    111\u001b[0m         headers,\n\u001b[0;32m    112\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Anwender\\anaconda3\\envs\\RAG\\lib\\site-packages\\httpcore\\_sync\\http11.py:177\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 177\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n",
      "File \u001b[1;32mc:\\Users\\Anwender\\anaconda3\\envs\\RAG\\lib\\site-packages\\httpcore\\_sync\\http11.py:231\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    230\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mServer disconnected without sending a response.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 231\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RemoteProtocolError(msg)\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mreceive_data(data)\n",
      "\u001b[1;31mRemoteProtocolError\u001b[0m: Server disconnected without sending a response.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRemoteProtocolError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[99], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(time\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m, time\u001b[38;5;241m.\u001b[39mlocaltime()))\n\u001b[1;32m----> 2\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43minvoke_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCan you tell me if Beiersdorf conducts animal testing?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mchat_history\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mchat_history\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m chat_history \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat_history\u001b[39m\u001b[38;5;124m\"\u001b[39m] \n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMetadata: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompany\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtopic\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[98], line 251\u001b[0m, in \u001b[0;36minvoke_graph\u001b[1;34m(graph, question, chat_history, db_client, document_db, product_db, history_db)\u001b[0m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;124;03mRun the state graph with the given question and chat history.\u001b[39;00m\n\u001b[0;32m    229\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;124;03m        history_db (QdrantVectorStore): The history vector store to use.\u001b[39;00m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    240\u001b[0m state \u001b[38;5;241m=\u001b[39m State(\n\u001b[0;32m    241\u001b[0m     chat_history\u001b[38;5;241m=\u001b[39mchat_history,\n\u001b[0;32m    242\u001b[0m     question\u001b[38;5;241m=\u001b[39mquestion,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    249\u001b[0m     history_db\u001b[38;5;241m=\u001b[39mhistory_db,\n\u001b[0;32m    250\u001b[0m )\n\u001b[1;32m--> 251\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Anwender\\anaconda3\\envs\\RAG\\lib\\site-packages\\langgraph\\pregel\\__init__.py:2688\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[1;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, **kwargs)\u001b[0m\n\u001b[0;32m   2686\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2687\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m-> 2688\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream(\n\u001b[0;32m   2689\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   2690\u001b[0m     config,\n\u001b[0;32m   2691\u001b[0m     stream_mode\u001b[38;5;241m=\u001b[39mstream_mode,\n\u001b[0;32m   2692\u001b[0m     output_keys\u001b[38;5;241m=\u001b[39moutput_keys,\n\u001b[0;32m   2693\u001b[0m     interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before,\n\u001b[0;32m   2694\u001b[0m     interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after,\n\u001b[0;32m   2695\u001b[0m     debug\u001b[38;5;241m=\u001b[39mdebug,\n\u001b[0;32m   2696\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2697\u001b[0m ):\n\u001b[0;32m   2698\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   2699\u001b[0m         latest \u001b[38;5;241m=\u001b[39m chunk\n",
      "File \u001b[1;32mc:\\Users\\Anwender\\anaconda3\\envs\\RAG\\lib\\site-packages\\langgraph\\pregel\\__init__.py:2340\u001b[0m, in \u001b[0;36mPregel.stream\u001b[1;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[0;32m   2334\u001b[0m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[0;32m   2335\u001b[0m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates.\u001b[39;00m\n\u001b[0;32m   2336\u001b[0m     \u001b[38;5;66;03m# Channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[0;32m   2337\u001b[0m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[0;32m   2338\u001b[0m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps.\u001b[39;00m\n\u001b[0;32m   2339\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtick(input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels):\n\u001b[1;32m-> 2340\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mtick(\n\u001b[0;32m   2341\u001b[0m             loop\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[0;32m   2342\u001b[0m             timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[0;32m   2343\u001b[0m             retry_policy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_policy,\n\u001b[0;32m   2344\u001b[0m             get_waiter\u001b[38;5;241m=\u001b[39mget_waiter,\n\u001b[0;32m   2345\u001b[0m         ):\n\u001b[0;32m   2346\u001b[0m             \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[0;32m   2347\u001b[0m             \u001b[38;5;28;01myield from\u001b[39;00m output()\n\u001b[0;32m   2348\u001b[0m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Anwender\\anaconda3\\envs\\RAG\\lib\\site-packages\\langgraph\\pregel\\runner.py:158\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[1;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[0;32m    156\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 158\u001b[0m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m                \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweakref\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m                \u001b[49m\u001b[43mreraise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32mc:\\Users\\Anwender\\anaconda3\\envs\\RAG\\lib\\site-packages\\langgraph\\pregel\\retry.py:40\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[1;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[0;32m     38\u001b[0m     task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     42\u001b[0m     ns: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "File \u001b[1;32mc:\\Users\\Anwender\\anaconda3\\envs\\RAG\\lib\\site-packages\\langgraph\\utils\\runnable.py:606\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    602\u001b[0m config \u001b[38;5;241m=\u001b[39m patch_config(\n\u001b[0;32m    603\u001b[0m     config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq:step:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    604\u001b[0m )\n\u001b[0;32m    605\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 606\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[1;32mc:\\Users\\Anwender\\anaconda3\\envs\\RAG\\lib\\site-packages\\langgraph\\utils\\runnable.py:371\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    370\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[1;32m--> 371\u001b[0m         ret \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    372\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse:\n\u001b[0;32m    373\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "Cell \u001b[1;32mIn[98], line 182\u001b[0m, in \u001b[0;36mgenerate\u001b[1;34m(state)\u001b[0m\n\u001b[0;32m    178\u001b[0m docs_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(doc\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    179\u001b[0m messages \u001b[38;5;241m=\u001b[39m prompt\u001b[38;5;241m.\u001b[39minvoke({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m], \n\u001b[0;32m    180\u001b[0m                           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m: docs_content,\n\u001b[0;32m    181\u001b[0m                           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat_history\u001b[39m\u001b[38;5;124m\"\u001b[39m: state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat_history\u001b[39m\u001b[38;5;124m\"\u001b[39m]})\n\u001b[1;32m--> 182\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m: response\u001b[38;5;241m.\u001b[39mcontent}\n",
      "File \u001b[1;32mc:\\Users\\Anwender\\anaconda3\\envs\\RAG\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:310\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m    299\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    300\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    305\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    306\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[0;32m    307\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    308\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[0;32m    309\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatGeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m--> 310\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    311\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[0;32m    312\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    313\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    314\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    315\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    316\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    317\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    318\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    319\u001b[0m         )\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    320\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[1;32mc:\\Users\\Anwender\\anaconda3\\envs\\RAG\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:859\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    850\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m    851\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    852\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    856\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    857\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    858\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_messages, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Anwender\\anaconda3\\envs\\RAG\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:694\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    691\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[0;32m    692\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    693\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 694\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_with_cache(\n\u001b[0;32m    695\u001b[0m                 m,\n\u001b[0;32m    696\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    697\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[i] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    698\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    699\u001b[0m             )\n\u001b[0;32m    700\u001b[0m         )\n\u001b[0;32m    701\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    702\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[1;32mc:\\Users\\Anwender\\anaconda3\\envs\\RAG\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:925\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    923\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    924\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 925\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m    926\u001b[0m             messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    927\u001b[0m         )\n\u001b[0;32m    928\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    929\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Anwender\\anaconda3\\envs\\RAG\\lib\\site-packages\\langchain_ollama\\chat_models.py:705\u001b[0m, in \u001b[0;36mChatOllama._generate\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_generate\u001b[39m(\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    700\u001b[0m     messages: List[BaseMessage],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    703\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    704\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatResult:\n\u001b[1;32m--> 705\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_chat_stream_with_aggregation(\n\u001b[0;32m    706\u001b[0m         messages, stop, run_manager, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    707\u001b[0m     )\n\u001b[0;32m    708\u001b[0m     generation_info \u001b[38;5;241m=\u001b[39m final_chunk\u001b[38;5;241m.\u001b[39mgeneration_info\n\u001b[0;32m    709\u001b[0m     chat_generation \u001b[38;5;241m=\u001b[39m ChatGeneration(\n\u001b[0;32m    710\u001b[0m         message\u001b[38;5;241m=\u001b[39mAIMessage(\n\u001b[0;32m    711\u001b[0m             content\u001b[38;5;241m=\u001b[39mfinal_chunk\u001b[38;5;241m.\u001b[39mtext,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    716\u001b[0m         generation_info\u001b[38;5;241m=\u001b[39mgeneration_info,\n\u001b[0;32m    717\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Anwender\\anaconda3\\envs\\RAG\\lib\\site-packages\\langchain_ollama\\chat_models.py:642\u001b[0m, in \u001b[0;36mChatOllama._chat_stream_with_aggregation\u001b[1;34m(self, messages, stop, run_manager, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_chat_stream_with_aggregation\u001b[39m(\n\u001b[0;32m    634\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    635\u001b[0m     messages: List[BaseMessage],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    639\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    640\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatGenerationChunk:\n\u001b[0;32m    641\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 642\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterate_over_stream(messages, stop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    643\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m final_chunk \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    644\u001b[0m             final_chunk \u001b[38;5;241m=\u001b[39m chunk\n",
      "File \u001b[1;32mc:\\Users\\Anwender\\anaconda3\\envs\\RAG\\lib\\site-packages\\langchain_ollama\\chat_models.py:727\u001b[0m, in \u001b[0;36mChatOllama._iterate_over_stream\u001b[1;34m(self, messages, stop, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_iterate_over_stream\u001b[39m(\n\u001b[0;32m    721\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    722\u001b[0m     messages: List[BaseMessage],\n\u001b[0;32m    723\u001b[0m     stop: Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    724\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    725\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[ChatGenerationChunk]:\n\u001b[0;32m    726\u001b[0m     is_thinking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 727\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m stream_resp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_stream(messages, stop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    728\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(stream_resp, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    729\u001b[0m             chunk \u001b[38;5;241m=\u001b[39m ChatGenerationChunk(\n\u001b[0;32m    730\u001b[0m                 message\u001b[38;5;241m=\u001b[39mAIMessageChunk(\n\u001b[0;32m    731\u001b[0m                     content\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    744\u001b[0m                 ),\n\u001b[0;32m    745\u001b[0m             )\n",
      "File \u001b[1;32mc:\\Users\\Anwender\\anaconda3\\envs\\RAG\\lib\\site-packages\\langchain_ollama\\chat_models.py:629\u001b[0m, in \u001b[0;36mChatOllama._create_chat_stream\u001b[1;34m(self, messages, stop, **kwargs)\u001b[0m\n\u001b[0;32m    626\u001b[0m chat_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_chat_params(messages, stop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chat_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m--> 629\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mchat(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mchat_params)\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mchat(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mchat_params)\n",
      "File \u001b[1;32mc:\\Users\\Anwender\\anaconda3\\envs\\RAG\\lib\\site-packages\\ollama\\_client.py:163\u001b[0m, in \u001b[0;36mClient._request.<locals>.inner\u001b[1;34m()\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minner\u001b[39m():\n\u001b[1;32m--> 163\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mstream(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mas\u001b[39;00m r:\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m       r\u001b[38;5;241m.\u001b[39mraise_for_status()\n",
      "File \u001b[1;32mc:\\Users\\Anwender\\anaconda3\\envs\\RAG\\lib\\contextlib.py:135\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Anwender\\anaconda3\\envs\\RAG\\lib\\site-packages\\httpx\\_client.py:868\u001b[0m, in \u001b[0;36mClient.stream\u001b[1;34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[0m\n\u001b[0;32m    845\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    846\u001b[0m \u001b[38;5;124;03mAlternative to `httpx.request()` that streams the response body\u001b[39;00m\n\u001b[0;32m    847\u001b[0m \u001b[38;5;124;03minstead of loading it into memory at once.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    853\u001b[0m \u001b[38;5;124;03m[0]: /quickstart#streaming-responses\u001b[39;00m\n\u001b[0;32m    854\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    855\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_request(\n\u001b[0;32m    856\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[0;32m    857\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    866\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mextensions,\n\u001b[0;32m    867\u001b[0m )\n\u001b[1;32m--> 868\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    872\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    873\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    875\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\Anwender\\anaconda3\\envs\\RAG\\lib\\site-packages\\httpx\\_client.py:914\u001b[0m, in \u001b[0;36mClient.send\u001b[1;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[0;32m    910\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_timeout(request)\n\u001b[0;32m    912\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[1;32m--> 914\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    919\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    920\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    921\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[1;32mc:\\Users\\Anwender\\anaconda3\\envs\\RAG\\lib\\site-packages\\httpx\\_client.py:942\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[1;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[0;32m    939\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[0;32m    941\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 942\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    947\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    948\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Anwender\\anaconda3\\envs\\RAG\\lib\\site-packages\\httpx\\_client.py:979\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[1;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[0;32m    976\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    977\u001b[0m     hook(request)\n\u001b[1;32m--> 979\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    980\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    981\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\Anwender\\anaconda3\\envs\\RAG\\lib\\site-packages\\httpx\\_client.py:1014\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m   1009\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1010\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1011\u001b[0m     )\n\u001b[0;32m   1013\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[1;32m-> 1014\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[0;32m   1018\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[1;32mc:\\Users\\Anwender\\anaconda3\\envs\\RAG\\lib\\site-packages\\httpx\\_transports\\default.py:249\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhttpcore\u001b[39;00m\n\u001b[0;32m    237\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[0;32m    238\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    239\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    247\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[0;32m    248\u001b[0m )\n\u001b[1;32m--> 249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m    250\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool\u001b[38;5;241m.\u001b[39mhandle_request(req)\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n",
      "File \u001b[1;32mc:\\Users\\Anwender\\anaconda3\\envs\\RAG\\lib\\contextlib.py:153\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[1;34m(self, typ, value, traceback)\u001b[0m\n\u001b[0;32m    151\u001b[0m     value \u001b[38;5;241m=\u001b[39m typ()\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 153\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraceback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m value\n",
      "File \u001b[1;32mc:\\Users\\Anwender\\anaconda3\\envs\\RAG\\lib\\site-packages\\httpx\\_transports\\default.py:118\u001b[0m, in \u001b[0;36mmap_httpcore_exceptions\u001b[1;34m()\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m    117\u001b[0m message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(exc)\n\u001b[1;32m--> 118\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m mapped_exc(message) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[1;31mRemoteProtocolError\u001b[0m: Server disconnected without sending a response."
     ]
    }
   ],
   "source": [
    "print(time.strftime(\"%H:%M:%S\", time.localtime()))\n",
    "result = invoke_graph(graph=graph,\n",
    "                      question = \"Can you tell me if Beiersdorf conducts animal testing?\",\n",
    "                      chat_history = chat_history)\n",
    "chat_history = result[\"chat_history\"] \n",
    "\n",
    "print(f'Metadata: {result[\"query\"][\"company\"]}, {result[\"query\"][\"topic\"]}')\n",
    "print(f'Context: {result[\"context\"]}\\n\\n')\n",
    "print(f'Answer RAG: {result[\"answer\"]}\\n\\n')\n",
    "print(f'Chat history: {result[\"chat_history\"]}')\n",
    "\n",
    "print(time.strftime(\"%H:%M:%S\", time.localtime()))\n",
    "print(f'\\nAnswer LLM: {llm.invoke(\"Can you tell me if Beiersdorf conducts animal testing?\").content}')\n",
    "print(time.strftime(\"%H:%M:%S\", time.localtime()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "3cbadb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8d96b23e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf6f45e",
   "metadata": {},
   "source": [
    "## Test conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f4bf7127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "previous chat_history: \n",
      "Metadata: {'query': 'animal testing by Beiersdorf', 'company': 'Beiersdorf', 'topic': 'ethics'}\n",
      "Context: [Document(metadata={'company': 'Beiersdorf', 'topic': 'ethics', 'source_file': 'beiersdorf_ethics.json', '_id': 4, '_collection_name': 'demo_collection'}, page_content='Beiersdorf, the German multinational company behind brands like Nivea, Eucerin, and La Prairie, has a long-standing presence in the skincare industry. The companyâs ethical practices have been scrutinized in several areas, including animal testing, labor conditions, and corporate transparency. This report compiles verified information from both company sources and independent assessments. **Animal Testing:** Beiersdorf states on its official website that it does not conduct animal testing for its cosmetic products, except where required by law (e.g., in China, where regulatory authorities may require animal testing for certain products). The company emphasizes its commitment to alternative testing methods and supports the global ban on cosmetic animal testing. Beiersdorf is not listed as a certified cruelty-free brand by PETA or the Leaping Bunny program, as its compliance with Chinese regulations means some products may still be subject to animal testing in specific markets. However,'), Document(metadata={'company': 'Beiersdorf', 'topic': 'ethics', 'source_file': 'beiersdorf_ethics.json', '_id': 5, '_collection_name': 'demo_collection'}, page_content='certified cruelty-free brand by PETA or the Leaping Bunny program, as its compliance with Chinese regulations means some products may still be subject to animal testing in specific markets. However, the company is a member of the European Partnership for Alternative Approaches to Animal Testing (EPAA), reflecting its investment in non-animal research methods. **Labor Conditions:** Beiersdorf adheres to international labor standards and is a member of the United Nations Global Compact, committing to fair labor practices, human rights, and anti-corruption measures. The companyâs 2022 Sustainability Report highlights initiatives to ensure safe working conditions, fair wages, and employee development programs across its supply chain. Independent audits, such as those by the Sedex Members Ethical Trade Audit (SMETA), have been conducted for some of Beiersdorfâs suppliers, though comprehensive public data on all suppliers is limited. Reports from organizations like the Business & Human'), Document(metadata={'company': 'Beiersdorf', 'topic': 'ethics', 'source_file': 'beiersdorf_ethics.json', '_id': 7, '_collection_name': 'demo_collection'}, page_content='palm oil, which is certified by the Roundtable on Sustainable Palm Oil (RSPO)). Beiersdorfâs commitment to reducing CO2 emissions (aiming for carbon neutrality by 2025) and its use of recyclable packaging are noted as positive steps. **Independent Assessments:** While Beiersdorf is not listed by PETA or Leaping Bunny due to its presence in China, it has received praise for reducing animal testing reliance. The companyâs ESG scores from agencies like MSCI (AA rating) and Sustainalytics (low-risk rating) reflect strong governance and moderate ethical performance. However, ongoing scrutiny is recommended, especially regarding supply chain transparency and market-specific animal testing policies. Sources: Beiersdorf Sustainability Report 2022, UN Global Compact, CDP, EPAA, RSPO, MSCI, Sustainalytics, Business & Human Rights Resource Centre.'), Document(metadata={'company': 'Beiersdorf', 'topic': 'ethics', 'source_file': 'beiersdorf_ethics.json', '_id': 6, '_collection_name': 'demo_collection'}, page_content=\"Ethical Trade Audit (SMETA), have been conducted for some of Beiersdorfâs suppliers, though comprehensive public data on all suppliers is limited. Reports from organizations like the Business & Human Rights Resource Centre have not flagged major labor violations, but transparency regarding subcontractors remains an area for improvement. **Corporate Transparency:** Beiersdorf publishes annual sustainability reports detailing its environmental, social, and governance (ESG) performance. The company has received recognition for its transparency, including a 'B' rating from CDP (formerly the Carbon Disclosure Project) for climate change disclosure. However, critics argue that Beiersdorf could provide more granular data on its supply chain, particularly concerning raw material sourcing (e.g., palm oil, which is certified by the Roundtable on Sustainable Palm Oil (RSPO)). Beiersdorfâs commitment to reducing CO2 emissions (aiming for carbon neutrality by 2025) and its use of recyclable\")]\n",
      "\n",
      "\n",
      "Answer RAG: Based on the provided context, I can tell you that Beiersdorf conducts animal testing in certain circumstances. According to their official website, they do not conduct animal testing for their cosmetic products unless it is required by law, such as in China where regulatory authorities may require animal testing for certain products. However, this means that some of their products may still be subject to animal testing in specific markets.\n",
      "\n",
      "It's worth noting that Beiersdorf is a member of the European Partnership for Alternative Approaches to Animal Testing (EPAA), which reflects its investment in non-animal research methods. Additionally, they have received praise from organizations like PETA and Leaping Bunny for reducing their reliance on animal testing.\n",
      "\n",
      "So, while Beiersdorf does not conduct animal testing as a standard practice, there are exceptions where it may be required by law or in specific markets.\n",
      "\n",
      "\n",
      "Chat history: Here's a summarized version of our conversation:\n",
      "\n",
      "**Summary:**\n",
      "Beiersdorf conducts animal testing only when it is required by law in specific markets, such as China, where regulatory authorities may require animal testing for certain products.\n",
      "\n",
      "Note that I've removed any conflicting information and focused on the most recent statement.\n"
     ]
    }
   ],
   "source": [
    "result = invoke_graph(graph=graph,\n",
    "                      question = \"Can you tell me if Beiersdorf conducts animal testing?\",\n",
    "                      chat_history = chat_history)\n",
    "\n",
    "print(f'previous chat_history: {chat_history}')\n",
    "\n",
    "print(f'Metadata: {result[\"query\"]}')\n",
    "print(f'Context: {result[\"context\"]}\\n\\n')\n",
    "print(f'Answer RAG: {result[\"answer\"]}\\n\\n')\n",
    "print(f'Chat history: {result[\"chat_history\"]}')\n",
    "chat_history = result[\"chat_history\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c02cfea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a summarized version of our conversation:\n",
      "\n",
      "**Summary:**\n",
      "Beiersdorf conducts animal testing only when it is required by law in specific markets, such as China, where regulatory authorities may require animal testing for certain products.\n",
      "\n",
      "Note that I've removed any conflicting information and focused on the most recent statement.\n",
      "previous chat_history: Here's a summarized version of our conversation:\n",
      "\n",
      "**Summary:**\n",
      "Beiersdorf conducts animal testing only when it is required by law in specific markets, such as China, where regulatory authorities may require animal testing for certain products.\n",
      "\n",
      "Note that I've removed any conflicting information and focused on the most recent statement.\n",
      "Metadata: {'query': 'Environmental policies of Beiersdorf', 'company': 'Beiersdorf', 'topic': 'environment'}\n",
      "Context: [Document(metadata={'company': 'Beiersdorf', 'topic': 'environment', 'source_file': 'beiersdorf_env.json', '_id': 2, '_collection_name': 'demo_collection'}, page_content='MacArthur Foundation to advance circular economy principles. Water Stewardship: The company has reduced water consumption in its production processes by 35% since 2014, implementing water-saving technologies and recycling systems. Beiersdorf is a member of the Alliance for Water Stewardship (AWS) and has certified several sites, including its Hamburg factory, under the AWS Standard. Responsible Sourcing: Beiersdorf adheres to strict sourcing guidelines for raw materials, prioritizing certified sustainable ingredients. For example, 100% of its palm oil derivatives are RSPO (Roundtable on Sustainable Palm Oil)-certified, and it sources organic shea butter through fair trade partnerships. The company has also committed to zero deforestation in its supply chain by 2025. Independent audits by organizations like Rainforest Alliance verify these claims. Challenges and Criticisms: Despite progress, Beiersdorf faces challenges, such as the environmental impact of its global supply chain and'), Document(metadata={'company': 'Beiersdorf', 'topic': 'environment', 'source_file': 'beiersdorf_env.json', '_id': 0, '_collection_name': 'demo_collection'}, page_content=\"Beiersdorf, the German multinational company behind brands like Nivea, Eucerin, and La Prairie, has made significant commitments to environmental sustainability, which are outlined in its official reports and verified by independent sources. The company's sustainability strategy, 'Care Beyond Skin,' focuses on reducing its environmental footprint across its value chain, including climate action, sustainable packaging, and responsible sourcing. Climate Action: Beiersdorf has set ambitious climate targets aligned with the Science-Based Targets initiative (SBTi). The company aims to reduce its Scope 1 and 2 greenhouse gas (GHG) emissions by 30% by 2025 and achieve net-zero emissions by 2045. As of its 2022 Sustainability Report, Beiersdorf reduced its carbon emissions from operations by 64% compared to 2018, primarily through energy efficiency measures and renewable energy adoption. The company sources 100% renewable electricity for its production sites in Europe and has expanded this\"), Document(metadata={'company': 'Beiersdorf', 'topic': 'environment', 'source_file': 'beiersdorf_env.json', '_id': 3, '_collection_name': 'demo_collection'}, page_content='organizations like Rainforest Alliance verify these claims. Challenges and Criticisms: Despite progress, Beiersdorf faces challenges, such as the environmental impact of its global supply chain and the carbon footprint of its product transportation. Greenpeace has criticized the company for not fully eliminating microplastics from all its products, though Beiersdorf plans to phase them out by 2025. Sources: Beiersdorf AG Sustainability Report 2022, CDP Climate Change Report 2022, Ellen MacArthur Foundation case studies, RSPO Annual Communications of Progress 2022, Greenpeace report on microplastics (2021).'), Document(metadata={'company': 'Beiersdorf', 'topic': 'environment', 'source_file': 'beiersdorf_env.json', '_id': 1, '_collection_name': 'demo_collection'}, page_content=\"compared to 2018, primarily through energy efficiency measures and renewable energy adoption. The company sources 100% renewable electricity for its production sites in Europe and has expanded this initiative globally. Independent verification by CDP (formerly the Carbon Disclosure Project) awarded Beiersdorf an 'A-' score for climate change mitigation in 2022, reflecting its transparency and action. Sustainable Packaging: Beiersdorf has committed to making 100% of its packaging reusable, recyclable, or compostable by 2025. As of 2022, 89% of its packaging met these criteria. The company has also reduced plastic use by introducing refill systems and lightweight packaging, such as the Nivea Naturally Good face care line, which uses 50% less plastic. Beiersdorf collaborates with the Ellen MacArthur Foundation to advance circular economy principles. Water Stewardship: The company has reduced water consumption in its production processes by 35% since 2014, implementing water-saving\")]\n",
      "\n",
      "\n",
      "Answer RAG: Beiersdorf, the German multinational behind brands like Nivea, Eucerin, and La Prairie, has made significant commitments to environmental sustainability. Their strategy, 'Care Beyond Skin,' focuses on reducing their environmental footprint across their value chain, including climate action, sustainable packaging, and responsible sourcing.\n",
      "\n",
      "**Climate Action:**\n",
      "Beiersdorf has set ambitious targets to reduce greenhouse gas emissions, aiming for a 30% decrease in Scope 1 and 2 emissions by 2025, with the goal of achieving net-zero emissions by 2045. As of their 2022 Sustainability Report, they have reduced carbon emissions from operations by 64% compared to 2018 through energy efficiency measures and renewable energy adoption. They source 100% renewable electricity for production sites in Europe and are expanding this globally.\n",
      "\n",
      "**Sustainable Packaging:**\n",
      "Beiersdorf has committed to making 100% of their packaging reusable, recyclable, or compostable by 2025. As of 2022, 89% of their packaging met these criteria. They have introduced refill systems and lightweight packaging, such as the Nivea Naturally Good face care line, which uses 50% less plastic.\n",
      "\n",
      "**Responsible Sourcing:**\n",
      "The company adheres to strict sourcing guidelines for raw materials, prioritizing certified sustainable ingredients. For example:\n",
      "\t* 100% of their palm oil derivatives are RSPO-certified.\n",
      "\t* They source organic shea butter through fair trade partnerships.\n",
      "\t* They have committed to zero deforestation in their supply chain by 2025, with independent audits verifying these claims.\n",
      "\n",
      "**Water Stewardship:**\n",
      "Beiersdorf has reduced water consumption in production processes by 35% since 2014 through water-saving technologies and recycling systems. They are a member of the Alliance for Water Stewardship (AWS) and have certified several sites under the AWS Standard, including their Hamburg factory.\n",
      "\n",
      "Overall, Beiersdorf's sustainability efforts are aligned with the Science-Based Targets initiative (SBTi), and they receive independent verification from organizations like CDP and Rainforest Alliance.\n",
      "\n",
      "\n",
      "Chat history: Here's a summarized version of our conversation:\n",
      "\n",
      "**Summary:**\n",
      "Beiersdorf, the German multinational behind brands like Nivea, Eucerin, and La Prairie, has made significant commitments to environmental sustainability through its \"Care Beyond Skin\" strategy. Key highlights include:\n",
      "* Reducing greenhouse gas emissions by 30% by 2025 and aiming for net-zero emissions by 2045.\n",
      "* Committing to making 100% of their packaging reusable, recyclable, or compostable by 2025 (89% achieved as of 2022).\n",
      "* Prioritizing certified sustainable ingredients in sourcing, including RSPO-certified palm oil derivatives and fair trade partnerships.\n",
      "* Reducing water consumption by 35% since 2014 through various initiatives.\n",
      "\n",
      "Note that I've removed any conflicting information and focused on the most recent statement.\n"
     ]
    }
   ],
   "source": [
    "result = invoke_graph(graph=graph,\n",
    "                      question = \"Can you tell me about any of the companies environmental policies?\",\n",
    "                      chat_history = chat_history)\n",
    "\n",
    "print(f'previous chat_history: {chat_history}')\n",
    "\n",
    "print(f'Metadata: {result[\"query\"]}')\n",
    "print(f'Context: {result[\"context\"]}\\n\\n')\n",
    "print(f'Answer RAG: {result[\"answer\"]}\\n\\n')\n",
    "print(f'Chat history: {result[\"chat_history\"]}')\n",
    "chat_history = result[\"chat_history\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4f0b9ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a summarized version of our conversation:\n",
      "\n",
      "**Summary:**\n",
      "Beiersdorf, the German multinational behind brands like Nivea, Eucerin, and La Prairie, has made significant commitments to environmental sustainability through its \"Care Beyond Skin\" strategy. Key highlights include:\n",
      "* Reducing greenhouse gas emissions by 30% by 2025 and aiming for net-zero emissions by 2045.\n",
      "* Committing to making 100% of their packaging reusable, recyclable, or compostable by 2025 (89% achieved as of 2022).\n",
      "* Prioritizing certified sustainable ingredients in sourcing, including RSPO-certified palm oil derivatives and fair trade partnerships.\n",
      "* Reducing water consumption by 35% since 2014 through various initiatives.\n",
      "\n",
      "Note that I've removed any conflicting information and focused on the most recent statement.\n",
      "previous chat_history: Here's a summarized version of our conversation:\n",
      "\n",
      "**Summary:**\n",
      "Beiersdorf, the German multinational behind brands like Nivea, Eucerin, and La Prairie, has made significant commitments to environmental sustainability through its \"Care Beyond Skin\" strategy. Key highlights include:\n",
      "* Reducing greenhouse gas emissions by 30% by 2025 and aiming for net-zero emissions by 2045.\n",
      "* Committing to making 100% of their packaging reusable, recyclable, or compostable by 2025 (89% achieved as of 2022).\n",
      "* Prioritizing certified sustainable ingredients in sourcing, including RSPO-certified palm oil derivatives and fair trade partnerships.\n",
      "* Reducing water consumption by 35% since 2014 through various initiatives.\n",
      "\n",
      "Note that I've removed any conflicting information and focused on the most recent statement.\n",
      "Metadata: {'query': \"Loreal's animal testing policy\", 'company': 'Beiersdorf', 'topic': 'environment'}\n",
      "Context: [Document(metadata={'company': 'Beiersdorf', 'topic': 'environment', 'source_file': 'beiersdorf_env.json', '_id': 3, '_collection_name': 'demo_collection'}, page_content='organizations like Rainforest Alliance verify these claims. Challenges and Criticisms: Despite progress, Beiersdorf faces challenges, such as the environmental impact of its global supply chain and the carbon footprint of its product transportation. Greenpeace has criticized the company for not fully eliminating microplastics from all its products, though Beiersdorf plans to phase them out by 2025. Sources: Beiersdorf AG Sustainability Report 2022, CDP Climate Change Report 2022, Ellen MacArthur Foundation case studies, RSPO Annual Communications of Progress 2022, Greenpeace report on microplastics (2021).'), Document(metadata={'company': 'Beiersdorf', 'topic': 'environment', 'source_file': 'beiersdorf_env.json', '_id': 2, '_collection_name': 'demo_collection'}, page_content='MacArthur Foundation to advance circular economy principles. Water Stewardship: The company has reduced water consumption in its production processes by 35% since 2014, implementing water-saving technologies and recycling systems. Beiersdorf is a member of the Alliance for Water Stewardship (AWS) and has certified several sites, including its Hamburg factory, under the AWS Standard. Responsible Sourcing: Beiersdorf adheres to strict sourcing guidelines for raw materials, prioritizing certified sustainable ingredients. For example, 100% of its palm oil derivatives are RSPO (Roundtable on Sustainable Palm Oil)-certified, and it sources organic shea butter through fair trade partnerships. The company has also committed to zero deforestation in its supply chain by 2025. Independent audits by organizations like Rainforest Alliance verify these claims. Challenges and Criticisms: Despite progress, Beiersdorf faces challenges, such as the environmental impact of its global supply chain and'), Document(metadata={'company': 'Beiersdorf', 'topic': 'environment', 'source_file': 'beiersdorf_env.json', '_id': 0, '_collection_name': 'demo_collection'}, page_content=\"Beiersdorf, the German multinational company behind brands like Nivea, Eucerin, and La Prairie, has made significant commitments to environmental sustainability, which are outlined in its official reports and verified by independent sources. The company's sustainability strategy, 'Care Beyond Skin,' focuses on reducing its environmental footprint across its value chain, including climate action, sustainable packaging, and responsible sourcing. Climate Action: Beiersdorf has set ambitious climate targets aligned with the Science-Based Targets initiative (SBTi). The company aims to reduce its Scope 1 and 2 greenhouse gas (GHG) emissions by 30% by 2025 and achieve net-zero emissions by 2045. As of its 2022 Sustainability Report, Beiersdorf reduced its carbon emissions from operations by 64% compared to 2018, primarily through energy efficiency measures and renewable energy adoption. The company sources 100% renewable electricity for its production sites in Europe and has expanded this\"), Document(metadata={'company': 'Beiersdorf', 'topic': 'environment', 'source_file': 'beiersdorf_env.json', '_id': 1, '_collection_name': 'demo_collection'}, page_content=\"compared to 2018, primarily through energy efficiency measures and renewable energy adoption. The company sources 100% renewable electricity for its production sites in Europe and has expanded this initiative globally. Independent verification by CDP (formerly the Carbon Disclosure Project) awarded Beiersdorf an 'A-' score for climate change mitigation in 2022, reflecting its transparency and action. Sustainable Packaging: Beiersdorf has committed to making 100% of its packaging reusable, recyclable, or compostable by 2025. As of 2022, 89% of its packaging met these criteria. The company has also reduced plastic use by introducing refill systems and lightweight packaging, such as the Nivea Naturally Good face care line, which uses 50% less plastic. Beiersdorf collaborates with the Ellen MacArthur Foundation to advance circular economy principles. Water Stewardship: The company has reduced water consumption in its production processes by 35% since 2014, implementing water-saving\")]\n",
      "\n",
      "\n",
      "Answer RAG: Based on the provided context, I don't have any information about L'OrÃ©al's animal testing policies. The retrieved context only mentions Beiersdorf, a German multinational company behind brands like Nivea, Eucerin, and La Prairie, and its sustainability efforts. If you'd like to know more about L'OrÃ©al or provide additional context, I'll do my best to assist you.\n",
      "\n",
      "\n",
      "Chat history: Here's a single summary message:\n",
      "\n",
      "You asked about L'OrÃ©al's animal testing policies, but I only had information about Beiersdorf, a German multinational behind brands like Nivea, Eucerin, and La Prairie. You wanted to know more about this topic without conflicting information, so I provided the most recent statements about Beiersdorf's sustainability efforts under its \"Care Beyond Skin\" strategy, including reducing greenhouse gas emissions, making 100% of packaging reusable/recyclable/compostable, using certified sustainable ingredients, and reducing water consumption.\n"
     ]
    }
   ],
   "source": [
    "result = invoke_graph(graph=graph,\n",
    "                      question = \"Can you tell me about Loreals animal testing policies?\",\n",
    "                      chat_history = chat_history)\n",
    "\n",
    "print(f'previous chat_history: {chat_history}')\n",
    "\n",
    "print(f'Metadata: {result[\"query\"]}')\n",
    "print(f'Context: {result[\"context\"]}\\n\\n')\n",
    "print(f'Answer RAG: {result[\"answer\"]}\\n\\n')\n",
    "print(f'Chat history: {result[\"chat_history\"]}')\n",
    "chat_history = result[\"chat_history\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca6d554",
   "metadata": {},
   "source": [
    "### Clear chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6c670324",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_client.delete(collection_name=history_collection_name, points_selector=Filter(must=[FieldCondition(key=\"user_ID\", match=MatchValue(value=user_ID))])) # delete user chat history from database\n",
    "db_client.close() # close the database connection"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
